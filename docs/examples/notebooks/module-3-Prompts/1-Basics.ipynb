{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65c46b1",
   "metadata": {},
   "source": [
    "# Using Prompts Inline\n",
    "We need to frist complete the same basic steps we did in the other modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f793af",
   "metadata": {},
   "source": [
    "## 1 - Load the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad4b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f5f8a",
   "metadata": {},
   "source": [
    "## 2 - Load Azure OpenAI Endpoint details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be28908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set path to the .env file in the notebooks directory\n",
    "notebooks_dir = Path(os.path.abspath('')).parent.parent\n",
    "env_path = notebooks_dir / '.env'\n",
    "\n",
    "# Load environment variables from the notebooks/.env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Set up environment variables\n",
    "# You can also create a .env file with the following variables\n",
    "# AZURE_OPENAI_API_KEY=your-api-key\n",
    "# AZURE_OPENAI_ENDPOINT=your-endpoint\n",
    "\n",
    "# Check if environment variables are set\n",
    "def check_env_vars():\n",
    "    required_vars = [\"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_ENDPOINT\"]\n",
    "    missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(f\"Warning: Missing environment variables: {', '.join(missing_vars)}\")\n",
    "        print(f\"Please create a .env file at {env_path} with these variables.\")\n",
    "        print(\"Some examples in this notebook may not work without these variables.\")\n",
    "    else:\n",
    "        print(\"All required environment variables are set!\")\n",
    "        \n",
    "        # Display part of the API key and endpoint for verification\n",
    "        api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "        endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "        \n",
    "        # Show only the first 5 and last 4 characters of the API key\n",
    "        if api_key:\n",
    "            masked_key = api_key[:5] + \"*\" * (len(api_key) - 9) + api_key[-4:] if len(api_key) > 9 else \"Not found\"\n",
    "            print(f\"API Key: {masked_key} (masked for security)\")\n",
    "            \n",
    "        # Show the endpoint domain\n",
    "        if endpoint:\n",
    "            print(f\"Endpoint: {endpoint}\")\n",
    "\n",
    "# Uncomment to check environment variables\n",
    "# check_env_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69becccd",
   "metadata": {},
   "source": [
    "## 3- Add Chat Completion Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe7718c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chat completion service using deployment: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "# Get values from environment variables\n",
    "deployment_name = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o\")\n",
    "endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# Add Azure OpenAI chat completion service\n",
    "chat_service = AzureChatCompletion(\n",
    "    deployment_name=deployment_name,\n",
    "    endpoint=endpoint,\n",
    "    api_key=api_key\n",
    "    # model_id=\"gpt-4\"  # Optional if deployment name matches model\n",
    ")\n",
    "\n",
    "# kernel.add_service(chat_service)\n",
    "\n",
    "print(f\"Added chat completion service using deployment: {deployment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f338942",
   "metadata": {},
   "source": [
    "## 4 - Construct the Kernel, ChatHistory and get instance of the ChatCompletion Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c3b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "# Get the chat service from kernel\n",
    "kernel.add_service(chat_service) \n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "        \n",
    "history = ChatHistory(system_message=\"You are a helpful assistant.\")\n",
    "history.add_user_message(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d33bf",
   "metadata": {},
   "source": [
    "## 5 - Let's create a prompt Template and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab23570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: Sure! Here's the Pig Latin translation of \"Why is the sky blue?\":\n",
      "\n",
      "**Pig Latin Translation:**  \n",
      "\"Ywhay isyay ethay kysay ueblay?\"\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "promptPiglatin = \"\"\"++++\n",
    "    Convert the follow to Pig Latin: \n",
    "    {{$input}}\n",
    "    ++++\n",
    "\n",
    "    Pig Latin Translation: \n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt template config\n",
    "prompt_template_piglatin = PromptTemplateConfig(\n",
    "    template=promptPiglatin,\n",
    "    name=\"PigLatin\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input to convert to Pig Latin\")\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "kernel.ask\n",
    "\n",
    "func = kernel.add_function(\n",
    "    function_name=\"pigLatinFunc\",\n",
    "    plugin_name=\"pigLatinPlugin\",\n",
    "    prompt_template_config=prompt_template_piglatin,\n",
    ")\n",
    "input_text = \"\"\"Why is the sky blue?\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Invoke the function\n",
    "result = await kernel.invoke(func, input=input_text)\n",
    "\n",
    "# Print the result\n",
    "print(\"AI Response:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
